{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2301e3b3-5ea6-41b6-a319-5bdcee76333b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64b38504-b56e-4a78-bbb6-3dd131b71558",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Lab: Adding Our Own Data to a Multi-Stage Reasoning System\n",
    "\n",
    "### Working with external knowledge bases \n",
    "In this notebook we're going to augment the knowledge base of our LLM with additional data. We will split the notebook into two halves:\n",
    "- First, we will walk through how to load in a relatively small, local text file using a `DocumentLoader`, split it into chunks, and store it in a vector database using `ChromaDB`.\n",
    "- Second, you will get a chance to show what you've learned by building a larger system with the complete works of Shakespeare. \n",
    "----\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Add external local data to your LLM's knowledge base via a vector database.\n",
    "2. Construct a Question-Answer(QA) LLMChain to \"talk to your data.\"\n",
    "3. Load external data sources from remote locations and store in a vector database.\n",
    "4. Leverage different retrieval methods to search over your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ad5ab4-c537-4c91-9c5a-f7b58d332e72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2b87eab-5fb8-4e1a-9b19-707eec7ec71b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| No action taken\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/large-language-models/v01\"\n\nValidating the locally installed datasets:\n| listing local files...(8 seconds)\n| validation completed...(8 seconds total)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing lab testing framework.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nUsing the \"default\" schema.\n\nPredefined paths variables:\n| DA.paths.working_dir: /dbfs/mnt/dbacademy-users/labuser4359064@vocareum.com/large-language-models\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/labuser4359064@vocareum.com/large-language-models/database.db\n| DA.paths.datasets:    /dbfs/mnt/dbacademy-datasets/large-language-models/v01\n\nSetup completed (22 seconds)\n\nThe models developed or used in this course are for demonstration and learning purposes only.\nModels may occasionally output offensive, inaccurate, biased information, or harmful instructions.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3be4f792-86dc-42ce-b24d-b5ed83d9550e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adcc2b2a-d5e3-4423-b69d-067861f8e8f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting chromadb==0.3.21\n  Downloading chromadb-0.3.21-py3-none-any.whl (46 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.4/46.4 kB 1.3 MB/s eta 0:00:00\nRequirement already satisfied: tiktoken==0.3.3 in /databricks/python3/lib/python3.10/site-packages (0.3.3)\nCollecting uvicorn[standard]>=0.18.3\n  Downloading uvicorn-0.23.1-py3-none-any.whl (59 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.5/59.5 kB 3.1 MB/s eta 0:00:00\nCollecting posthog>=2.4.0\n  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\nRequirement already satisfied: sentence-transformers>=2.2.2 in /databricks/python3/lib/python3.10/site-packages (from chromadb==0.3.21) (2.2.2)\nCollecting fastapi>=0.85.1\n  Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.7/65.7 kB 12.2 MB/s eta 0:00:00\nCollecting hnswlib>=0.7\n  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'done'\nRequirement already satisfied: pydantic>=1.9 in /databricks/python3/lib/python3.10/site-packages (from chromadb==0.3.21) (1.10.6)\nRequirement already satisfied: requests>=2.28 in /databricks/python3/lib/python3.10/site-packages (from chromadb==0.3.21) (2.28.1)\nCollecting numpy>=1.21.6\n  Using cached numpy-1.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\nCollecting clickhouse-connect>=0.5.7\n  Downloading clickhouse_connect-0.6.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (966 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 966.1/966.1 kB 26.4 MB/s eta 0:00:00\nCollecting duckdb>=0.7.1\n  Downloading duckdb-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.9/15.9 MB 59.3 MB/s eta 0:00:00\nRequirement already satisfied: pandas>=1.3 in /databricks/python3/lib/python3.10/site-packages (from chromadb==0.3.21) (1.4.4)\nRequirement already satisfied: regex>=2022.1.18 in /databricks/python3/lib/python3.10/site-packages (from tiktoken==0.3.3) (2022.7.9)\nCollecting zstandard\n  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 72.0 MB/s eta 0:00:00\nCollecting lz4\n  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 67.1 MB/s eta 0:00:00\nRequirement already satisfied: importlib-metadata in /databricks/python3/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (4.11.3)\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2022.1)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2022.9.14)\nRequirement already satisfied: urllib3>=1.26 in /databricks/python3/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (1.26.11)\nCollecting typing-extensions>=4.5.0\n  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\nCollecting starlette<0.28.0,>=0.27.0\n  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 11.1 MB/s eta 0:00:00\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.3.21) (2.8.2)\nCollecting monotonic>=1.5\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nCollecting backoff>=1.10.0\n  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from posthog>=2.4.0->chromadb==0.3.21) (1.16.0)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.3.21) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.3.21) (2.0.4)\nRequirement already satisfied: torch>=1.6.0 in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.13.1+cpu)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.64.1)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.28.1)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.1.1)\nRequirement already satisfied: torchvision in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (0.14.1+cpu)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (0.14.1)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.9.1)\nRequirement already satisfied: sentencepiece in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (0.1.97)\nRequirement already satisfied: nltk in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (3.7)\nCollecting h11>=0.8\n  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 9.1 MB/s eta 0:00:00\nRequirement already satisfied: click>=7.0 in /databricks/python3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (8.0.4)\nCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.1/4.1 MB 49.0 MB/s eta 0:00:00\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (6.0)\nCollecting python-dotenv>=0.13\n  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\nCollecting httptools>=0.5.0\n  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 428.8/428.8 kB 39.0 MB/s eta 0:00:00\nCollecting watchfiles>=0.13\n  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 73.3 MB/s eta 0:00:00\nCollecting websockets>=10.4\n  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.9/129.9 kB 20.1 MB/s eta 0:00:00\nRequirement already satisfied: packaging>=20.9 in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (21.3)\nRequirement already satisfied: fsspec in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (2022.7.1)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.6.0)\nCollecting anyio<5,>=3.4.0\n  Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.9/80.9 kB 13.2 MB/s eta 0:00:00\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /databricks/python3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.13.3)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata->clickhouse-connect>=0.5.7->chromadb==0.3.21) (3.8.0)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.10/site-packages (from nltk->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb==0.3.21) (2.2.0)\nCollecting numpy>=1.21.6\n  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 63.6 MB/s eta 0:00:00\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /databricks/python3/lib/python3.10/site-packages (from torchvision->sentence-transformers>=2.2.2->chromadb==0.3.21) (9.2.0)\nCollecting exceptiongroup\n  Downloading exceptiongroup-1.1.2-py3-none-any.whl (14 kB)\nCollecting sniffio>=1.1\n  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.0.9)\nBuilding wheels for collected packages: hnswlib\n  Building wheel for hnswlib (pyproject.toml): started\n  Building wheel for hnswlib (pyproject.toml): finished with status 'done'\n  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2158984 sha256=9c77f252e368ddf0560017516820358852178aad2af867db15ae4e42bb56c919\n  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\nSuccessfully built hnswlib\nInstalling collected packages: monotonic, duckdb, zstandard, websockets, uvloop, typing-extensions, sniffio, python-dotenv, numpy, lz4, httptools, h11, exceptiongroup, backoff, uvicorn, posthog, hnswlib, clickhouse-connect, anyio, watchfiles, starlette, fastapi, chromadb\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.3.0\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-04580e70-521a-4040-bfc6-12bc55e8a859\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-04580e70-521a-4040-bfc6-12bc55e8a859\n    Can't uninstall 'numpy'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\ndatabricks-feature-store 0.12.1 requires pyspark<4,>=3.1.2, which is not installed.\nydata-profiling 4.1.2 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.4 which is incompatible.\nnumba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.24.4 which is incompatible.\nmleap 0.20.0 requires scikit-learn<0.23.0,>=0.22.0, but you have scikit-learn 1.1.1 which is incompatible.\nSuccessfully installed anyio-3.7.1 backoff-2.2.1 chromadb-0.3.21 clickhouse-connect-0.6.7 duckdb-0.8.1 exceptiongroup-1.1.2 fastapi-0.100.0 h11-0.14.0 hnswlib-0.7.0 httptools-0.6.0 lz4-4.3.2 monotonic-1.6 numpy-1.24.4 posthog-3.0.1 python-dotenv-1.0.0 sniffio-1.3.0 starlette-0.27.0 typing-extensions-4.7.1 uvicorn-0.23.1 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3 zstandard-0.21.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install chromadb==0.3.21 tiktoken==0.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a16f91f-f80b-4940-922c-42e450e6577e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Fill in your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ebadc4a-dea7-4a59-87ef-0cf623295d43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# For many of the services that we'll using in the notebook, we'll need a HuggingFace API key so this cell will ask for it:\n",
    "# HuggingFace Hub: https://huggingface.co/inference-api\n",
    "\n",
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '<MY_HUGGINGFACEHUB_API_TOKEN>'\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0a7e55-f937-45f8-aafe-1de0114ed758",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Building a Personalized Document Oracle\n",
    "\n",
    "In this notebook, we're going to build a special type of LLMChain that will enable us to ask questions of our data. We will be able to \"speak to our data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fbcfd2a-d3ff-45c1-b1ce-97f73ff4a49e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 1 - Loading Documents into our Vector Store\n",
    "For this system we'll leverage the [ChromaDB vector database](https://www.trychroma.com/) and load in some text we have on file. This file is of a hypothetical laptop being reviewed in both long form and with brief customer reviews. We'll use LangChain's `TextLoader` to load this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c4f9b5f-22f6-4652-a018-9e2294dd092c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Raytech Supernova Laptop Review: A Star in the Making\\nIntroduction\\nThe laptop market has become increasingly competitive in recent years, with countless manufacturers vying for consumer attention. Raytech, a relatively new player in the game, has recently released the Supernova laptop, a device that aims to establish itself among the giants of the industry. In this comprehensive review, we will delve into every aspect of the Raytech Supernova laptop, covering its design, performance, features, and value for money. Let\\'s find out if this newcomer has what it takes to make an impact in the crowded market.\\nDesign and Build Quality\\nThe first thing you\\'ll notice about the Raytech Supernova is its sleek, modern design. The laptop is encased in a premium, brushed aluminum chassis with a matte finish, lending it an air of sophistication. It\\'s a lightweight device, weighing in at just 2.8 pounds, making it easy to carry around for those always on the go. The slim profile, measuring 0.6 inches in thickness, adds to its portability.\\nThe Supernova\\'s build quality is impressive, with no flexing or creaking when handling the device. The hinge is sturdy and smooth, allowing for easy adjustment of the display while keeping it stable during use. The laptop\\'s keyboard is well-spaced, offering a comfortable typing experience. The keys are backlit, with customizable lighting options, making it convenient for use in dimly lit environments.\\nDisplay and Graphics \\nThe Raytech Supernova comes with a 15.6-inch 4K UHD (3840 x 2160) IPS display, offering crisp and vibrant visuals. The screen is capable of producing a wide color gamut, ensuring accurate color reproduction across different media types. The panel has a matte finish, which helps to reduce glare and reflections, making it ideal for use in various lighting conditions.\\nThe laptop is powered by an NVIDIA GeForce RTX 3070 GPU, which provides excellent graphics performance for gaming and other demanding tasks. With support for real-time ray tracing and DLSS, the Supernova is well-suited for graphic-intensive applications and games. The GPU performance ensures smooth and immersive gameplay, even at high settings.\\nPerformance and Battery Life \\nUnder the hood, the Raytech Supernova is powered by the latest 11th Gen Intel Core i7 processor, paired with 16GB of DDR4 RAM. This combination ensures snappy performance during everyday tasks, such as web browsing and productivity applications. The laptop also has a 1TB NVMe SSD, which offers fast read/write speeds, resulting in quick boot times and application launches.\\nIn our tests, the Supernova managed to handle intensive tasks, such as video editing and 3D rendering, with ease. Even when pushed to its limits, the laptop remained cool and quiet, thanks to its efficient cooling system.\\nBattery life is an essential aspect of any laptop, and the Raytech Supernova does not disappoint. The device comes with a 97Wh battery, which, in our testing, lasted for around 10 hours of continuous web browsing and productivity tasks. When used for gaming or other demanding tasks, the battery life is reduced to approximately 5 hours, which is still impressive for a high-performance laptop.\\nConnectivity and Ports\\nThe Raytech Supernova offers a wide range of connectivity options, ensuring compatibility with various peripherals and devices. On the left side, you\\'ll find a USB 3.2 Gen 2 Type-A port, an HDMI 2.1 port, and a Gigabit Ethernet port. On the right side, there\\'s a Thunderbolt 4 port, two USB 3.2 Gen 1 Type-A ports, a 3.5mm audio jack, and an SD card reader. The Thunderbolt 4 port supports Power Delivery, allowing you to charge the laptop and connect peripherals with a single cable.\\nThe Supernova also comes with Wi-Fi 6 and Bluetooth 5.1, ensuring fast and reliable wireless connections. These features make the laptop versatile, allowing you to connect multiple devices and peripherals simultaneously without any hassle.\\nAudio and Webcam \\nThe audio quality on the Raytech Supernova is impressive, thanks to its built-in stereo speakers. The laptop features Dolby Atmos audio technology, which enhances the audio experience by providing immersive sound quality. The speakers deliver clear and crisp audio, with a reasonable amount of bass for a laptop. However, for a more immersive experience, external speakers or headphones are recommended.\\nThe Supernova is equipped with a 720p HD webcam, which is adequate for video calls and conferencing. The camera provides decent image quality under good lighting conditions but struggles in low light situations. The built-in dual-array microphones, however, offer clear and noise-free audio capture during calls.\\nSoftware and Security \\nThe Raytech Supernova comes pre-installed with Windows 10 Home, offering a familiar and user-friendly operating system. A free upgrade to Windows 11 is available, which introduces new features and improvements to the overall user experience.\\nIn terms of security, the Supernova includes a fingerprint reader integrated into the power button. This feature allows for quick and secure logins using Windows Hello. The laptop also features a TPM 2.0 chip, which provides hardware-based encryption for sensitive data, further enhancing the device\\'s security.\\nCustomer Support and Warranty\\nRaytech offers a standard one-year limited warranty for the Supernova laptop, covering manufacturing defects and hardware issues. The company provides customer support through email, live chat, and phone, ensuring that users have access to assistance when needed.\\nIn our interactions with Raytech\\'s customer support, we found the representatives to be knowledgeable, friendly, and responsive. The company also maintains an online support portal, which includes a comprehensive knowledge base, software downloads, and troubleshooting guides for common issues.\\nConclusion\\nThe Raytech Supernova is a compelling laptop that offers an impressive combination of performance, features, and design. With its sleek aluminum chassis, vibrant 4K display, and powerful hardware, the Supernova stands out in the crowded laptop market.\\nThe laptop\\'s excellent battery life, wide range of connectivity options, and robust security features make it a versatile device, suitable for both professional and personal use. While the audio and webcam quality could be improved, these are minor drawbacks in an otherwise outstanding laptop.\\nOverall, the Raytech Supernova offers excellent value for money, making it an ideal choice for those looking for a high-performance laptop that doesn\\'t compromise on style or functionality. If you\\'re in the market for a new laptop, the Raytech Supernova should definitely be on your shortlist.\\n\\nCustomer Reviews:\\n\"Sleek and powerful - 5 stars\"\\nI recently purchased the Raytech Supernova and I couldn\\'t be happier. It\\'s lightweight, stylish, and powerful, making it perfect for both work and play. The 4K display is stunning, and the battery life is impressive. Highly recommended!\\n\\n\"Great performance but average webcam - 4 stars\"\\nThe Raytech Supernova has exceeded my expectations in terms of performance and design. However, the webcam quality is just average. It works fine for casual video calls, but for professional use, I\\'d recommend an external webcam.\\n\\n\"Perfect for content creators - 5 stars\"\\nAs a video editor, the Supernova has been a game-changer for me. The 4K display, powerful GPU, and fast SSD make editing large video files a breeze. The connectivity options are also a plus. Absolutely love this laptop!\\n\\n\"Impressive gaming laptop - 5 stars\"\\nThe Raytech Supernova handles all my favorite games with ease, even on high settings. The display is beautiful, and the cooling system keeps the laptop quiet during long gaming sessions. A fantastic choice for gamers!\\n\\n\"Good but not perfect - 4 stars\"\\nI love the design, performance, and battery life of the Supernova. The only downside is the audio quality from the built-in speakers. It\\'s decent, but for a better experience, I use headphones or external speakers.\\n\\n\"Excellent value for money - 5 stars\"\\nThe Supernova offers great performance and features at a reasonable price. It\\'s sleek, lightweight, and powerful, making it suitable for both work and entertainment. Highly recommended for anyone in need of a new laptop!\\n\\n\"Great for on-the-go professionals - 4.5 stars\"\\nAs a traveling professional, the Supernova has been a reliable companion. Its lightweight design and long battery life make it ideal for working on the go. The only minor issue is the webcam quality, but overall, it\\'s an excellent laptop.\\n\\n\"Stylish and versatile - 5 stars\"\\nThe Raytech Supernova is a perfect combination of style and functionality. The aluminum chassis gives it a premium feel, and the performance is top-notch. The variety of ports allows me to connect all my peripherals without a problem. Highly satisfied!\\n\\n\"Reliable and user-friendly - 4 stars\"\\nThe Supernova has been a dependable laptop for my daily tasks. The 4K display is a treat for the eyes, and the performance is reliable. The fingerprint reader for secure login is a nice touch. However, the audio quality could be better.\\n\\n\"A solid choice for students - 5 stars\"\\nAs a student, I needed a laptop that could handle multitasking, media consumption, and occasional gaming. The Supernova checks all those boxes while being lightweight and stylish. The battery life is also a huge plus. I\\'m extremely happy with my purchase!\\n\\n', metadata={'source': '/dbfs/mnt/dbacademy-datasets/large-language-models/v01/reviews/fake_laptop_reviews.txt'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "import pandas as pd\n",
    "# We have some fake laptop reviews that we can load in\n",
    "laptop_reviews = TextLoader(f\"{DA.paths.datasets}/reviews/fake_laptop_reviews.txt\", encoding=\"utf8\")\n",
    "document = laptop_reviews.load()\n",
    "\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f14e73b4-2588-4095-b62b-af194c5ce844",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 2 - Chunking and Embeddings\n",
    "\n",
    "Now that we have the data in document format, we will split data into chunks using a `CharacterTextSplitter` and embed this data using Hugging Face's embedding LLM to embed this data for our vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f4310d-c2d8-4aa8-afda-a7b1821df772",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 6702, which is longer than the specified 250\nCreated a chunk of size 285, which is longer than the specified 250\nCreated a chunk of size 278, which is longer than the specified 250\nCreated a chunk of size 260, which is longer than the specified 250\nCreated a chunk of size 254, which is longer than the specified 250\nCreated a chunk of size 258, which is longer than the specified 250\nCreated a chunk of size 286, which is longer than the specified 250\nCreated a chunk of size 286, which is longer than the specified 250\nCreated a chunk of size 275, which is longer than the specified 250\nCreated a chunk of size 295, which is longer than the specified 250\nUsing embedded DuckDB with persistence: data will be stored in: /dbfs/mnt/dbacademy-users/labuser4359064@vocareum.com/large-language-models\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# First we split the data into manageable chunks to store as vectors. There isn't an exact way to do this, more chunks means more detailed context, but will increase the size of our vectorstore.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=250, chunk_overlap=10)\n",
    "texts = text_splitter.split_documents(document)\n",
    "# Now we'll create embeddings for our document so we can store it in a vector store and feed the data into an LLM. We'll use the sentence-transformers model for out embeddings. https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/ \n",
    "model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    cache_folder=DA.paths.datasets)  # Use a pre-cached model\n",
    "# Finally we make our Index using chromadb and the embeddings LLM\n",
    "chromadb_index = Chroma.from_documents(texts, embeddings, persist_directory=DA.paths.working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d6a5a9-6473-4b55-821f-973924b9defd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 3 - Creating our Document QA LLM Chain\n",
    "With our data now in vector form we need an LLM and a chain to take our queries and create tasks for our LLM to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e400073-a4b8-4f61-94d2-b613f349d9ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0fe261052d43ce8cb135d125dba91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/huggingface_hub/file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in /dbfs/mnt/dbacademy-datasets/large-language-models/v01. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n  warnings.warn(message)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0789420f9e69458f9e36f1b51c7b7c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbe1ece2ff44ec494b9853a0a579fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128cb769d7dc4d75800d0fa502dd95b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a77b01c058e439da84e0587c27680cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe7c5c6cdfa45a3a1ebbfcd9a1217ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5244b638b765425389dd98e666834d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# We want to make this a retriever, so we need to convert our index.  This will create a wrapper around the functionality of our vector database so we can search for similar documents/chunks in the vectorstore and retrieve the results:\n",
    "retriever = chromadb_index.as_retriever()\n",
    "\n",
    "# This chain will be used to do QA on the document. We will need\n",
    "# 1 - A LLM to do the language interpretation\n",
    "# 2 - A vector database that can perform document retrieval\n",
    "# 3 - Specification on how to deal with this data (more on this soon)\n",
    "\n",
    "hf_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/flan-t5-large\",\n",
    "    task=\"text2text-generation\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"max_length\": 4096,\n",
    "        \"cache_dir\": DA.paths.datasets,\n",
    "    },\n",
    ")\n",
    "\n",
    "chain_type = \"refine\"  # Options: stuff, map_reduce, refine, map_rerank\n",
    "laptop_qa = RetrievalQA.from_chain_type(\n",
    "    llm=hf_llm, chain_type=\"refine\", retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cbaea75-8d43-48b3-8058-899d62879ffe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 4 - Talking to Our Data\n",
    "Now we are ready to send prompts to our LLM and have it use our prompt, the access to our data, and read the information, process, and return with a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1be1be30-1f1a-4c32-b1da-2276979d4de0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1477 > 512). Running this sequence through the model will result in indexing errors\n/databricks/python/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "'Raytech Supernova'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's ask the chain about the product we have.\n",
    "laptop_name = laptop_qa.run(\n",
    "    \"What is the full name of the laptop?\"\n",
    ")\n",
    "display(laptop_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf396b5c-e0cb-43ed-aee4-c2cb12c7a479",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "'15.6-inch 4K UHD (3840 x 2160) IPS display'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we'll ask the chain about the product.\n",
    "laptop_features = laptop_qa.run(\n",
    "    \"What are some of the laptop's features?\"\n",
    ")\n",
    "display(laptop_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ae7697-c989-435e-b5e5-ce80a7e07e82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finally let's ask the chain about the reviews.\n",
    "laptop_reviews = laptop_qa.run(\n",
    "    \"What is the general sentiment of the reviews?\"\n",
    ")\n",
    "display(laptop_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d328c8f-7976-40fd-bce9-9d4d66a5e907",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise: Working with larger documents\n",
    "This document was relatively small. So let's see if we can work with something bigger. To show how well we can scale the vector database, let's load in a larger document. For this we'll get data from the [Gutenberg Project](https://www.gutenberg.org/) where thousands of free-to-access texts. We'll use the complete works of William Shakespeare.\n",
    "\n",
    "Instead of a local text document, we'll download the complete works of Shakespeare using the `GutenbergLoader` that works with the Gutenberg project: https://www.gutenberg.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdae1632-665f-43a8-ab01-0a3d915e257e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GutenbergLoader\n",
    "\n",
    "loader = GutenbergLoader(\n",
    "    \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
    ")  # Complete works of Shakespeare in a txt file\n",
    "\n",
    "all_shakespeare_text = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b419451-44da-4c21-8b83-9d044d2c1806",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Now it's your turn! Based on what we did previously, fill in the missing parts below to build your own QA LLMChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82166538-7a29-4d86-a9c5-3d17c4234600",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: /dbfs/mnt/dbacademy-users/labuser4359064@vocareum.com/large-language-models\n"
     ]
    }
   ],
   "source": [
    "# STEP #1: split text -> embedd -> send to Chroma Vector DB\n",
    "text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(all_shakespeare_text)\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "  model_name=model_name,\n",
    "  cache_folder=DA.paths.datasets\n",
    ")\n",
    "docsearch = Chroma.from_documents(texts, embeddings, persist_directory=DA.paths.working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41d600b-7563-454b-b5da-c02100efcf38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson3, question1\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion3_1(embeddings, docsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e7787c-7099-4351-92ab-5100c037c6cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 2\n",
    "\n",
    "Let's see if we can do what we did with the laptop reviews. \n",
    "\n",
    "Think about what is likely to happen now. Will this command succeed? \n",
    "\n",
    "(***Hint: think about the maximum sequence length of a model***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc027cd-cf23-4da7-9263-ef1d38907dd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Hamlet (play) is a 16th-century English dramatist'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "retriever = docsearch.as_retriever()\n",
    "\n",
    "hf_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/flan-t5-large\",\n",
    "    task=\"text2text-generation\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"max_length\": 9800,\n",
    "        \"cache_dir\": DA.paths.datasets,\n",
    "    },\n",
    ")\n",
    "\n",
    "chain_type = \"map_reduce\"  # Options: stuff, map_reduce, refine, map_rerank\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=hf_llm, chain_type=\"refine\", retriever=retriever\n",
    ")\n",
    "\n",
    "# Let's start with the simplest method: \"Stuff\" which puts all of the data into the prompt and asks a question of it:\n",
    "#qa = RetrievalQA.from_chain_type(\"stuff\")\n",
    "query = \"Who is the main character in the play Hamlet?\"\n",
    "\n",
    "query_results_hamlet = qa.run(\n",
    "     query\n",
    ")\n",
    "\n",
    "query_results_hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d76db6-33e1-40ad-b733-512eefaf2740",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson3, question2\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion3_2(qa, query_results_hamlet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed6f249-0284-4327-84b9-b6eae380418c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 3\n",
    "\n",
    "Now that we're working with larger documents, we should be mindful of the input sequence limitations that our LLM has. \n",
    "\n",
    "Chain Types for document loader:\n",
    "\n",
    "- [`stuff`](https://docs.langchain.com/docs/components/chains/index_related_chains#stuffing) - Stuffing is the simplest method, whereby you simply stuff all the related data into the prompt as context to pass to the language model.\n",
    "- [`map_reduce`](https://docs.langchain.com/docs/components/chains/index_related_chains#map-reduce) - This method involves running an initial prompt on each chunk of data (for summarization tasks, this could be a summary of that chunk; for question-answering tasks, it could be an answer based solely on that chunk).\n",
    "- [`refine`](https://docs.langchain.com/docs/components/chains/index_related_chains#refine) - This method involves running an initial prompt on the first chunk of data, generating some output. For the remaining documents, that output is passed in, along with the next document, asking the LLM to refine the output based on the new document.\n",
    "- [`map_rerank`](https://docs.langchain.com/docs/components/chains/index_related_chains#map-rerank) - This method involves running an initial prompt on each chunk of data, that not only tries to complete a task but also gives a score for how certain it is in its answer. The responses are then ranked according to this score, and the highest score is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "903def5b-a0c3-4fed-82b9-5a2cbbf71abe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'A MERCHANT, friend to Antipholus of Syracuse.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_type = \"refine\"  # Options: stuff, map_reduce, refine, map_rerank\n",
    "# qa = RetrievalQA.from_chain_type(\n",
    "#     llm=hf_llm, chain_type=chain_type, retriever=retriever\n",
    "# )\n",
    "\n",
    "# TODO\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "  llm=hf_llm, chain_type=chain_type, retriever=docsearch.as_retriever())\n",
    "query = \"Who is the main character in the Merchant of Venice?\"\n",
    "query_results_venice = qa.run(query)\n",
    " \n",
    "query_results_venice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b4cc4c-dd64-458e-92f9-382655833625",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson3, question3\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion3_3(qa, query_results_venice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09581d38-fad2-41cb-b623-22e261c261c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Romeo is belov’d, and loves again, Alike bewitched by the charm of looks; But to his foe suppos’d he must complain, And she steal love’s sweet bait from fearful hooks: Being held a foe, he may not have access'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "#That's much better! Let's try another type\n",
    "chain_type = \"refine\"  # Options: stuff, map_reduce, refine, map_rerank\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=hf_llm, chain_type=chain_type, retriever=retriever\n",
    ")\n",
    "query = \"What happens to romeo and juliet?\"\n",
    "query_results_romeo = qa.run(query)\n",
    " \n",
    "query_results_romeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654a858f-804e-42d4-b0b1-5b68fa495206",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson3, question4\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion3_4(qa, query_results_romeo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52489f3f-ac29-47a0-acc0-6428118c8f11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Submit your Results!\n",
    "\n",
    "To get credit for this lab, click the submit button to report the results. If you run into any issues, click `Run` -> `Clear state and run all`, and make sure all tests have passed before re-submitting. If you accidentally deleted any tests, take a look at the notebook's version history to recover them or reload the notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af2bd49c-9b39-4ab3-b948-7f1cd3983419",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM 03L - Building LLM Chains Lab",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
